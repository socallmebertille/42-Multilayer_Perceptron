<div align="center" class="text-center">
  <h1>42-Multilayer_Perceptron</h1>
  
  <img alt="last-commit" src="https://img.shields.io/github/last-commit/socallmebertille/42-Multilayer_Perceptron?style=flat&amp;logo=git&amp;logoColor=white&amp;color=0080ff" class="inline-block mx-1" style="margin: 0px 2px;">
  <img alt="repo-top-language" src="https://img.shields.io/github/languages/top/socallmebertille/42-Multilayer_Perceptron?style=flat&amp;color=0080ff" class="inline-block mx-1" style="margin: 0px 2px;">
  <img alt="repo-language-count" src="https://img.shields.io/github/languages/count/socallmebertille/42-Multilayer_Perceptron?style=flat&amp;color=0080ff" class="inline-block mx-1" style="margin: 0px 2px;">
  <p><em>Built with the tools and technologies:</em></p>
  <img alt="Markdown" src="https://img.shields.io/badge/Markdown-000000.svg?style=flat&amp;logo=Markdown&amp;logoColor=white" class="inline-block mx-1" style="margin: 0px 2px;">
  <img alt="GNU%20Bash" src="https://img.shields.io/badge/GNU%20Bash-4EAA25.svg?style=flat&amp;logo=GNU-Bash&amp;logoColor=white" class="inline-block mx-1" style="margin: 0px 2px;">
  <img alt="Python" src="https://img.shields.io/badge/python-2496ED.svg?style=flat&amp;logo=python&amp;logoColor=white" class="inline-block mx-1" style="margin: 0px 2px;">
</div>

<h2>Table of Contents</h2>
<ul class="list-disc pl-4 my-0">
  <li class="my-0"><a href="#overview">Overview</a></li>
  <ul class="list-disc pl-4 my-0">
    <li class="my-0"><a href="#install-a-great-virtual-environment">Install a great virtual environment</a></li>
    <li class="my-0"><a href="#mathematical-concept">Mathematical concept</a></li>
  </ul>
  <li class="my-0"><a href="#build-a-multilayer-perceptron-mlp">Build a Multilayer Perceptron (MLP)</a>
  <ul class="list-disc pl-4 my-0">
    <li class="my-0"><a href="#architecture">Architecture</a></li>
    <li class="my-0"><a href="#usage">Usage</a></li>
  </ul>
  </li>
</ul>

<h2>Overview</h2>
<h3>Install a great virtual environment</h3>

```bash
uv venv                                         # creation
source .venv/bin/activate                       # activation

uv pip install numpy                            # installation of dependencies
```

<h3>Mathematical concept</h3>

#### Multilayer Perceptron

> **Definition:** feedforward (information flows from the input layer to the output layer only) neural network model with at least 1-2 hidden layers.

MLP stacks several perceptrons organized in **layers**, each of which:
- takes the output of the previous layer
- transforms the space
- simplifies the problem

> **Universal approximation theorem:** a combination of simple functions can approximate any complex function.

#### Standard ML Pipeline

```mermaid
graph LR
    A[ðŸ“Š Raw Data] --> B[ðŸ“ Normalization]
    B --> C[âœ‚ï¸ Split train/validation/test]
    C --> D[âœ‚ï¸ Batching]
    D --> E[ðŸŽ¯ Forward pass = prediction]
    E --> F[ðŸ“ˆ Loss function]
    F --> G{Threshold or Early stopping ?}
    G --> |Yes| H[âœ… Best Model]
    G --> |No| I[ðŸ”„ Backpropagation = gradient]
    I --> J[ðŸ”„ Gradient descent = MAJ gradients]
```

#### Formulas

Each layer applies a weighted sum + activation :

**1. Forward Pass**

For each batch, for each layer $l$ :

$$a^{(l)} = f(z^{(l)}) = f(W^{(l)} a^{(l-1)} + b^{(l)})$$

Where:
- $l$ = layer
- $W^{(l)}$ = weight matrix
- $a^{(0)}$ = $x$ (input)
- $a^{(l-1)}$ = output of previous layer
- $b^{(l)}$ = bias
- $f$ = activation function (ReLU, sigmoidâ€¦)

> Each layer applies a weighted sum + activation

**Activation Functions:**

| Components | Sigmoid | Softmax | Linear (ReLU) |
| --- | --- | --- | --- |
| Output range | (0, 1) | (0, 1), sum to 1 | [0, +âˆž) |
| Use case | Binary / multi-label independent | Multi-class (mutually exclusive) | Hidden layers |
| Output structure | Single probability per neuron | Probability distribution | Non-linear activation |
| Advantages | Interpretable | Probabilistic | Simple, fast |
| Disadvantages | Saturation | Coupled to CE | Dead neurons |
| **Formula** | $\sigma(z) = \frac{1}{1 + e^{-z}}$ | $\text{softmax}(z_K) = \frac{e^{z_K}}{\sum_{j=1}^K e^{z_j}}$ | $\text{ReLU}(z) = \max(0, z)$ |

**Usage Rules:**
- **Hidden layers**: ReLU (standard), sometimes Sigmoid/Tanh
- **Output layer**:
  - Regression: Linear (no activation)
  - Binary classification: Sigmoid
  - Multi-class classification: Softmax

**2. Loss Function**

Measures the error: $L(\hat{y}, y)$

| Problem type | Loss function | Formula |
| --- | --- | --- |
| Binary classification | Binary Cross-Entropy | $\text{BCE} = -[y \log(\hat{y}) + (1-y) \log(1-\hat{y})]$ |
| Multi-class classification | Categorical Cross-Entropy | $\text{CCE} = -\sum_i y_i \log(\hat{y}_i)$ |
| Regression | Mean Squared Error | $\text{MSE} = \frac{1}{2m} \sum_i (\hat{y}_i - y_i)^2$ |

**3. Backpropagation**

Computes how each weight contributed to the error, by propagating gradients from output to input.

One layer: $a^{(l)} = f(z^{(l)})$

Layer error:
$$\delta^{(l)} = \frac{\partial L}{\partial z^{(l)}}$$

**Gradient Computation:**

| Element | Gradient |
| --- | --- |
| **Output layer** | $\delta^{(L)} = \hat{y} - y$ (Softmax+CE or Sigmoid+BCE) |
| **Hidden layer** | $\delta^{(l)} = (W^{(l+1)T} \delta^{(l+1)}) \odot f'(z^{(l)})$ |

**4. Gradient Descent**

| Element | Formula |
| --- | --- |
| Weight gradient | $\frac{\partial L}{\partial W^{(l)}} = \delta^{(l)} a^{(l-1)T}$ |
| Bias gradient | $\frac{\partial L}{\partial b^{(l)}} = \delta^{(l)}$ |
| Update | $W^{(l)} \leftarrow W^{(l)} - \eta \frac{\partial L}{\partial W^{(l)}}$ |

New weights = old weights - (learning rate Ã— weight gradient)

**5. Stopping Criteria**

- Maximum epochs reached
- Loss < minimal threshold (e.g., loss < 0.001)
- **Early stopping** (validation loss plateaus)
- Gradient â‰ˆ 0 (convergence reached)


<h2>Build a Multilayer Perceptron (MLP)</h2>
<h3>Architecture</h3>

```
multilayer-perceptron/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ network_config.txt # Exemple de config
â”œâ”€â”€ datasets/              # Created with the splitting flag
â”‚   â”œâ”€â”€ test_set.csv
â”‚   â”œâ”€â”€ train_set.csv
â”‚   â””â”€â”€ valid_set.csv
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ activations.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ losses.py
â”‚   â”œâ”€â”€ my_mlp.py          # Class MLP
â”‚   â”œâ”€â”€ parsing.py
â”‚   â”œâ”€â”€ preprocessing.py
â”‚   â”œâ”€â”€ split_data.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ test/                  # Script bash testeur 
|   â”œâ”€â”€ cli_parsing.sh
|   â”œâ”€â”€ config_parsing.sh
|   â”œâ”€â”€ file_management.sh
|   â””â”€â”€ mlp_training.sh
â”œâ”€â”€ mlp.py                 # Main entry point (lightweight)
â””â”€â”€ saved_model.npy        # Model saved by the training flag

```

<h3>Usage</h3>

```bash
usage: mlp.py [-h] --dataset DATASET [--split SPLIT]
              [--predict PREDICT] [--config CONFIG]
              [--layer LAYER [LAYER ...]] [--epochs EPOCHS]
              [--learning_rate LEARNING_RATE] [--batch_size BATCH_SIZE]
              [--loss {binaryCrossentropy,categoricalCrossentropy}]
              [--input_size INPUT_SIZE] [--output_size OUTPUT_SIZE]
              [--activation_hidden {sigmoid,relu,tanh}]
              [--activation_output {sigmoid,softmax,linear}]
              [--weights_init {heUniform,heNormal,xavierUniform,random}]

Multilayer Perceptron for binary classification

options:
  -h, --help            show this help message and exit
  --dataset DATASET     Path to dataset CSV file
  --split SPLIT         Split ratio (format: train,valid). Ex: 0.7,0.15
  --predict PREDICT     Path to saved model for prediction
  --config CONFIG       Path to config file (.txt)
  --layer LAYER [LAYER ...]
                        Hidden layer sizes âˆˆ â„•*. Ex: --layer 24 24 24
  --epochs EPOCHS       Number of training epochs âˆˆ â„•*
  --learning_rate LEARNING_RATE
                        Learning rate âˆˆ [0, 1]
  --batch_size BATCH_SIZE
                        Batch size âˆˆ â„•*
  --loss {binaryCrossentropy,categoricalCrossentropy}
                        Loss function
  --input_size INPUT_SIZE
                        Input size âˆˆ [1, +inf] (number of features)
  --output_size OUTPUT_SIZE
                        Output size âˆˆ [1, +inf] (number of classes)
  --activation_hidden {sigmoid,relu,tanh}
                        Activation function for hidden layers
  --activation_output {sigmoid,softmax,linear}
                        Activation function for output layer
  --weights_init {heUniform,heNormal,xavierUniform,random}
                        Weights initialization method
```
